For my final project, I chose to inspect and go through the data mining pipeline on a dataset involving Diabetes diagnoses for patients from a research study performed in an Iraqi hospital. Some issues with this dataset are that there are only 1000 observations and there is an imbalance in the target class as there are over 800 instances of one class of the target variable, barely 100 of another and around 50 of the final class. This class imbalance will be addressed during the modeling portion of the project. This bias toward the one class is baked in as the research included mostly older and typically unhealthy participants, which increases the likelihood of a positive diagnosis. Other than that, I think this dataset has some informative and useful features to look at and consider which will allow me to draw some concrete conclusions in the end. The original dataset I wanted to use was simply hospital records of patients that were involved with Diabetes in some way shape or form and there were over 100,000 instances and over 50 features. I was eager to use this due to the availability of data instances but most of the features were uninteresting to me and were not relevant to my hypothesis around general health and Diabetes, as most of the features were just general data you would collect from a patient like days in the hospital, doses of certain medications, not entirely useful to me.

The packages I used were numpy, pandas, seaborn, matplotlib, plotly, sklearn, and tensorflow.

Here is a link to where I sourced the data : https://b2find.dkrz.de/dataset/5540c7d9-5ed8-5c52-aca3-c1bf07ef6ec3

For my modeling portion of the project, I chose a couple of classifying methods that I thought would work best for my circumstances, and I also used grid search to try and tune hyperparameters of some of these methods to better fit the data. I began by using some ensemble methods to try and deal with the class imbalance, so I used random forest, and gradient boosting to begin. I then did a gridsearch to try and find the optimal hyperparameters to pair with these ensemble methods, but unfortunately the best performances in the grid search results for both were inferior to the classifiers with default hyperparameters. After this, I attempted to utilize oversampling using SMOTE (Synthetic Minority Oversampling Technique) with random forest which gave us our best predictive performance out of all of the considered methods to that point. I then used SMOTE in tandem with gradient boosting, which did not garner superior results, and finally with the decisiion tree classifier, which also did not yield anything special. Below are the cells in which the machine learning is performed.

To begin this discussion I would like to point out that most of the difficulty surrounding this project stemmed from my poor choice in dataset. The dataset was relatively clean and ready for modeling usage but the class imbalance and low number of observations makes it challenging for me or anyone to say whether the model would predict accurately with a more balanced dataset featuring the same variables and features, and that is an important thing to consider as the chosen patients for the research/experiment clearly occupied a certain demographic (older, generally unhealthy) and probably already had begun the process of screening for Diabetes prior to participating in the research, hence why over 80% of the observations resulted in positive diagnoses. For future considerations I would look for a dataset with similar features but more observations and a more balanced representation for the target class. This caveat to the dataset though allowed me to experiment with classifiers and techniques that I was unfamiliar with like random forest and gradient boosting, as well as SMOTE, which we just recently learned about in class luckily. Something that I found surprising about the project was the futility of using gridsearch with random forest and gradient boosting, I did not expect the default parameters to perform the best but I guess sometimes tuning frankly isn't necessary, as generic random forest classifier with SMOTE was able to achieve an area under the ROC curve of 1 for all three classes. Another thing I found surprising was the importance of certain features, as I predicted that Age, BMI and VLDL would be the most connected to the diagnosis, but HBA1C was by far the most important followed by Age and BMI.

Overall, as expected age and BMI are some of the leading variables in determining the diagnosis for Diabetes but there is one feature that is above them both: HBA1C. I actually know what an A1C calculation is as I have the testing done for it on me every 6 months when I go to the endocrinologist and essentially it tells you how high or low your blood sugar has been on average over the past 2-3 months. For example, typical range is from 5 aall the way up to 12 where 5 is an average blood sugar of 97 mg/dL, 6 is 126 mg/dL, 7 is 154 mg/dL, 8 is 183 mg/dL, ... 12 is 298 mg/dL. This is a reading of interest because individuals with Diabetes either have a nonfunctioning pancreas that can't produce insulin like myself or are insulin resistant and need to inject more insulin or take medication to mitigate the resistance, so people with untreated Diabetes will naturally have a higher HBA1C as the glucose in their bloodstream is not metabolized due to lack of insulin and contrarily individuals with a healthy pancreas and no issues concerning insulin absorption will have a lower HBA1C as the glucose in their blood is properly metabolized. Knowing this, we can now understand why HBA1C is of utmost importance of all features when it comes to determining the Diabetes diagnosis of a patient. Other than that, age and BMI are undeniably the best indicators of the remaining features, which also makes sense because as you get older your metabolism slows down and other hormones are altered as well which can lead to insulin resistance outright and BMI is generally a good at-a-glance metric to determine how appropriate an individual's weight is compared to their height, and as you become more overweight your metabolism and hormones change and with a higher fat to muscle ratio, glucose metabolism also deteriorates, leading to insulin resistance. Due to the dataset being relatively small and containing some very indicative features related to the target variable, the combination of SMOTE and a generic random forest classifier was able to predict the Diabetes diagnosis of patients to a very high degree based on the rest of the available health data and the aforementioned features. This model effectively mitigated the potential issues that class imbalance would otherwise have on performance through oversampling and the use of an ensemble classifier, which was also an expected outcome.

Video Link : https://youtu.be/KHkrQo_4p2g